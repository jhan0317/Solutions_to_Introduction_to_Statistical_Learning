t0 <- (b1.hat - 10) / se.b1.hat
b1.null.rej.sample[rep] <- (abs(t0) > t.crit)
t0 <- (b1.hat - 9) / se.b1.hat
b1.alt.rej.sample[rep] <- (abs(t0) > t.crit)
}
table (b1.ci.sample) / n.rep
# TRUE
#    1
# Part C: At each repetition of the simulation,          |
# calculate and record the values of the upper and lower |
# bounds of a 95% confidence interval for error          |
# variance; record whether the confidence interval       |
# surrounds the true value of the error variance,        |
# sig2 = 16. Once you have completed 500 simulations,    |
# calculate the relative frequency that the confidence   |
# interval surrounds the true value of the error         |
# variance. Is the numerical value of that relative      |
# frequency consistent with what you would expect?       |
# Explain.                                               |
table(sig2.ci.sample) / n.rep
# FALSE  TRUE
# 0.052 0.948
# Part D: At each repetition of the simulation, carry    |
# out a t test for the hypotheses                        |
#             H0: beta1 = 10 vs H0: beta1 ≠ 10           |
# controlling for type I error at level 0.05; record     |
# whether the test rejects or fails to reject H0. Once   |
# you have completed 500 simulations, calculate the      |
# relative frequency that the test rejects H0.           |
table (b1.null.rej.sample)/ n.rep
# FALSE
#     1
# Part E: At each repetition of the simulation, carry    |
# out a t test for the hypotheses                        |
#             H0: beta1 = 9 vs H0: beta1 ≠ 9             |
# controlling for type I error at level 0.05; record     |
# whether the test rejects or fails to reject H0. Once   |
# you have completed 500 simulations, propose an         |
# approximate value of the power of the test when the    |
# true value of the slope is beta1=10.                   |
table (b1.alt.rej.sample)/ n.rep
# FALSE
#     1
# Part F: Repeat Parts A-E, but this time specify the    |
# regressor values at 1.0, 2.0, ..., 10.0.               |
x <- seq(from=1, to=10, by=1.0)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
# Then, answer whether each of the following sumulation  |
# summaries                                              |
#    APPRECIABLY CHANGED or DID NOT APPRECIABLY CHANGE   |
# from the original version of simulation and explain    |
# why you could have expected that to be the case.       |
# (i.) The sample means of the simulated regression      |
# coefficients.                                          |
mean(b0.hat)
# [1] 50.05841
mean(b1.hat)
# [1] 9.993114
# DID NOT APPRECIABLY CHANGE.
# Why?
# (ii.) The sample variances of the simulated regression |
# coefficients.                                          |
var(b0.hat)
# [1] 2.378808
var(b1.hat)
# [1] 2.378808
b0.hat
var(b0.hat)
b0.hat
x <- seq(from=1, to=10, by=0.5)
# Next, randomly generate response values from the model |
#   y = 50 + 10 x + epsilon where epsilon ~ NID(0, 16)   |
# This may be achieved using the "rnorm" function, as in |
# the following code:                                    |
n <- length(x)
beta0 <- 50
beta1 <- 10
sig2 <- 16
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
# By repeatedly simulating regression data this way, we  |
# realize a process of *hypthetical repeated sampling*,  |
# which allows us to make sense of the theoretical       |
# properties of statistical procedures.                  |
# Complete the task in each part listed below based on   |
# 500 simulations of the regression model.               |
#                                                        |
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
n.rep <- 500
b0.hat <- numeric(length=n.rep)
b1.hat <- numeric(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
mean(b0.hat)
# [1] 49.77198
y.bar <- mean(y)
x.bar <- mean(x)
y.bar - mean(b1.hat)*x.bar
# [1] 48.73513
var(b0.hat)
# [1] 4.343748
sig2*(1/n.rep + mean(x)**2 / S.xx)      # Textbook page 19
# [1] 3.428491
mean(b1.hat)
# [1] 10.01684
S.xx <- sum((x-mean(x))^2)
S.xy <- sum((y-mean(y))*(x-mean(x)))
S.xy/S.xx
# [1] 9.655797
var(b1.hat)
# [1] 0.1176409
sig2 / S.xx
# [1] 0.1122807
b0.hat
mean(b0.hat)
mean(b1.hat)
var(b0.hat)
var(b1.hat)
x.bar <- mean(x)
y.bar <- mean(y)
y.bar - mean(b1.hat)*x.bar
sig2*(1/n.rep + mean(x)**2 / S.xx)      # Textbook page 19
# [1] 3.428491
S.xx <- sum((x-mean(x))^2)
S.xy <- sum((y-mean(y))*(x-mean(x)))
S.xy/S.xx
# [1] 9.655797
sig2 / S.xx
x <- seq(from=1, to=10, by=1.0)
b0.hat <- numeric(length=n.rep)
b1.hat <- numeric(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
# Then, answer whether each of the following sumulation  |
# summaries                                              |
#    APPRECIABLY CHANGED or DID NOT APPRECIABLY CHANGE   |
# from the original version of simulation and explain    |
# why you could have expected that to be the case.       |
# (i.) The sample means of the simulated regression      |
# coefficients.                                          |
mean(b0.hat)
# [1] 50.05841
mean(b1.hat)
# [1] 9.993114
# DID NOT APPRECIABLY CHANGE.
# Why?
# (ii.) The sample variances of the simulated regression |
# coefficients.                                          |
var(b0.hat)
# [1] 2.378808
var(b1.hat)
b0.hat
lm2
b0.hat
n.rep
coefficients(lm2)[[1]]
x <- seq(from=1, to=10, by=1.0)
b0.hat <- numeric(length=n.rep)
b1.hat <- numeric(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
length(y)
length(x)
beta0
beta1
rnorm(n, mean=0, sd = sqrt(sig2))
x <- seq(from=1, to=10, by=1.0)
n <- length(x)
b0.hat <- numeric(length=n.rep)
b1.hat <- numeric(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
mean(b0.hat)
# [1] 50.05841
mean(b1.hat)
# [1] 9.993114
# DID NOT APPRECIABLY CHANGE.
# Why?
# (ii.) The sample variances of the simulated regression |
# coefficients.                                          |
var(b0.hat)
# [1] 2.378808
var(b1.hat)
alpha <- 0.05
t.crit <- qt(alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
b0.hat.sample <- numeric(length=n.rep)
b1.hat.sample <- numeric(length=n.rep)
b1.ci.sample <- logical(length=n.rep)
sig2.ci.sample <- logical(length=n.rep)
b1.null.rej.sample <- logical(length=n.rep)
b1.alt.rej.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar <- mean(y)
S.xy <- sum((y-mean(y))*(x-mean(x)))
b1.hat <- S.xy/S.xx
b0.hat <- y.bar - b1.hat*x.bar
b0.hat.sample[rep] <- b0.hat
b1.hat.sample[rep] <- b1.hat
SS.T <- sum((y-mean(y))^2)
SS.Res <- SS.T - b1.hat*S.xy
MS.Res <- SS.Res/(n-2)
se.b1.hat <- sqrt(MS.Res / S.xx)
m.err <- t.crit*se.b1.hat
b1.ci.sample[rep] <- (abs(b1.hat - beta1) <= m.err)
sig2.ci.sample[rep] <- ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res /
chisq.crit.lo)
t0 <- (b1.hat - 10) / se.b1.hat
b1.null.rej.sample[rep] <- (abs(t0) > t.crit)
t0 <- (b1.hat - 9) / se.b1.hat
b1.alt.rej.sample[rep] <- (abs(t0) > t.crit)
}
table (b1.ci.sample) / n.rep
table(sig2.ci.sample) / n.rep
table (b1.null.rej.sample)/ n.rep
b1.hat
b0.hat.sample
alpha <- 0.05
t.crit <- qt(alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
b0.hat.sample <- numeric(length=n.rep)
b1.hat.sample <- numeric(length=n.rep)
b1.ci.sample <- logical(length=n.rep)
sig2.ci.sample <- logical(length=n.rep)
b1.null.rej.sample <- logical(length=n.rep)
b1.alt.rej.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
x.bar <- mean(x)
y.bar <- mean(y)
S.xy <- sum((y-mean(y))*(x-mean(x)))
b1.hat <- S.xy/S.xx
b0.hat <- y.bar - b1.hat*x.bar
b0.hat.sample[rep] <- b0.hat
b1.hat.sample[rep] <- b1.hat
SS.T <- sum((y-mean(y))^2)
SS.Res <- SS.T - b1.hat*S.xy
MS.Res <- SS.Res/(n-2)
se.b1.hat <- sqrt(MS.Res / S.xx)
m.err <- t.crit*se.b1.hat
b1.ci.sample[rep] <- (abs(b1.hat - beta1) <= m.err)
sig2.ci.sample[rep] <- ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res /
chisq.crit.lo)
t0 <- (b1.hat - 10) / se.b1.hat
b1.null.rej.sample[rep] <- (abs(t0) > t.crit)
t0 <- (b1.hat - 9) / se.b1.hat
b1.alt.rej.sample[rep] <- (abs(t0) > t.crit)
}
table (b1.ci.sample) / n.rep
b0.hat.sample
b1.hat.sample
alpha <- 0.05
t.crit <- qt(alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
b0.hat.sample <- numeric(length=n.rep)
b1.hat.sample <- numeric(length=n.rep)
b1.ci.sample <- logical(length=n.rep)
sig2.ci.sample <- logical(length=n.rep)
b1.null.rej.sample <- logical(length=n.rep)
b1.alt.rej.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
x.bar <- mean(x)
y.bar <- mean(y)
S.xy <- sum((y-mean(y))*(x-mean(x)))
S.xx <- sum((x-mean(x))**2)
b1.hat <- S.xy/S.xx
b0.hat <- y.bar - b1.hat*x.bar
b0.hat.sample[rep] <- b0.hat
b1.hat.sample[rep] <- b1.hat
SS.T <- sum((y-mean(y))^2)
SS.Res <- SS.T - b1.hat*S.xy
MS.Res <- SS.Res/(n-2)
se.b1.hat <- sqrt(MS.Res / S.xx)
m.err <- t.crit*se.b1.hat
b1.ci.sample[rep] <- (abs(b1.hat - beta1) <= m.err)
sig2.ci.sample[rep] <- ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res /
chisq.crit.lo)
t0 <- (b1.hat - 10) / se.b1.hat
b1.null.rej.sample[rep] <- (abs(t0) > t.crit)
t0 <- (b1.hat - 9) / se.b1.hat
b1.alt.rej.sample[rep] <- (abs(t0) > t.crit)
}
table (b1.ci.sample) / n.rep
# TRUE
#    1
# Part C: At each repetition of the simulation,          |
# calculate and record the values of the upper and lower |
# bounds of a 95% confidence interval for error          |
# variance; record whether the confidence interval       |
# surrounds the true value of the error variance,        |
# sig2 = 16. Once you have completed 500 simulations,    |
# calculate the relative frequency that the confidence   |
# interval surrounds the true value of the error         |
# variance. Is the numerical value of that relative      |
# frequency consistent with what you would expect?       |
# Explain.                                               |
table(sig2.ci.sample) / n.rep
# FALSE  TRUE
table (b1.ci.sample) / n.rep
table(sig2.ci.sample) / n.rep
table (b1.null.rej.sample)/ n.rep
table (b1.alt.rej.sample)/ n.rep
# FALSE
#     1
x <- seq(from=1, to=10, by=1.0)
n <- length(x)
b0.hat <- numeric(length=n.rep)
b1.hat <- numeric(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
lm2 <- lm(y ~ x)
b0.hat[rep] <- coefficients(lm2)[[1]]
b1.hat[rep] <- coefficients(lm2)[[2]]
}
# Then, answer whether each of the following sumulation  |
# summaries                                              |
#    APPRECIABLY CHANGED or DID NOT APPRECIABLY CHANGE   |
# from the original version of simulation and explain    |
# why you could have expected that to be the case.       |
# (i.) The sample means of the simulated regression      |
# coefficients.                                          |
mean(b0.hat)
# [1] 50.22606
mean(b1.hat)
# [1] 9.965315
# DID NOT APPRECIABLY CHANGE.
# Why?
# (ii.) The sample variances of the simulated regression |
# coefficients.                                          |
var(b0.hat)
# [1] 7.576319
var(b1.hat)
# [1] 0.1905237
b0.hat.sample <- numeric(length=n.rep)
b1.hat.sample <- numeric(length=n.rep)
b1.ci.sample <- logical(length=n.rep)
sig2.ci.sample <- logical(length=n.rep)
b1.null.rej.sample <- logical(length=n.rep)
b1.alt.rej.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
y <- beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
x.bar <- mean(x)
y.bar <- mean(y)
S.xy <- sum((y-mean(y))*(x-mean(x)))
S.xx <- sum((x-mean(x))**2)
b1.hat <- S.xy/S.xx
b0.hat <- y.bar - b1.hat*x.bar
b0.hat.sample[rep] <- b0.hat
b1.hat.sample[rep] <- b1.hat
SS.T <- sum((y-mean(y))^2)
SS.Res <- SS.T - b1.hat*S.xy
MS.Res <- SS.Res/(n-2)
se.b1.hat <- sqrt(MS.Res / S.xx)
m.err <- t.crit*se.b1.hat
b1.ci.sample[rep] <- (abs(b1.hat - beta1) <= m.err)
sig2.ci.sample[rep] <- ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res /
chisq.crit.lo)
t0 <- (b1.hat - 10) / se.b1.hat
b1.null.rej.sample[rep] <- (abs(t0) > t.crit)
t0 <- (b1.hat - 9) / se.b1.hat
b1.alt.rej.sample[rep] <- (abs(t0) > t.crit)
}
table (b1.ci.sample) / n.rep
x
table(sig2.ci.sample) / n.rep
table (b1.null.rej.sample)/ n.rep
m.err
auto <- read_csv(file = "Auto.csv")
auto <- read.csv(file = "Auto.csv")
setwd("~/Desktop/UVa/Courses/SYS6018/Exercises/Exercise2")
auto <- read.csv(file = "Auto.csv")
auto
auto.lm <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = auto)
summary(auto.lm)
auto$horsepower
plot(lm.fit)
plot(auto.lm.fit)
plot(auto.lm)
plot(auto.lm)
par(mfrow=c(2,2))
plot(auto.lm)
auto <- read.csv(file = "Auto.csv")
auto.lm <- lm(mpg ~ -name, data = auto)
par(mfrow=c(2,2))
plot(auto.lm)
auto <- read.csv(file = "Auto.csv")
auto.lm <- lm(mpg ~.-name, data = auto)
par(mfrow=c(2,2))
plot(auto.lm)
View(auto)
summary(lm(mpg∼cylinders*horsepower,data=auto))
summary(lm(mpg ∼ cylinders*horsepower,data=auto))
summary(lm(mpg ∼ cylinders+horsepower+cylinders:horsepower,data=auto))
summary(lm(mpg ~ cylinders+horsepower+cylinders:horsepower,data=auto))
corr(auto)
auto.corr()
cor(auto)
cor(auto[,:-name])
cor(auto[,:-1])
auto[:,:-1]
auto[-1]
auto[:-1]
auto[-auto[-1]]
auto[,-1]
auto[,-2]
auto[,-8]
auto[,-9]
cor(auto[,:-9])
cor(auto[,-9])
cor(subset(auto, select=-c(name)))
pairs(auto)
cor(subset(auto, select=-c(name)))
summary(lm(mpg~cylinders*displacement + displacement*weight))
summary(lm(mpg~cylinders*displacement + displacement*weight, data=auto))
attach(auto)
lm.fit3 = lm(mpg~log(weight)+sqrt(horsepower)+acceleration+I(acceleration^2))
attach(auto)
lm.fit3 = lm(mpg~log(weight)+acceleration+I(acceleration^2))
summary(lm.fit3)
par(mfrow=c(2,2))
plot(lm.fit3)
lm.fit2<-lm(log(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin,data=Auto)
lm.fit2<-lm(log(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin)
summary(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
attach(auto)
par(mfrow=c(2,2))
plot(lm(log(mpg) ~ . - name))
par(mfrow=c(2,2))
plot(lm(log(mpg) ~ . - name, data = auto))
set.seed(1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
x1=runif(100)
x1
x2
summary(lm(y~x1+x2))
cor(x1,x2)
plot(x1,x2)
anova(lm(y~x1+x2))
summary(lm(y~x1))
anova(lm(y~x1))
summary(lm(y~x2))
anova(lm(y~x2))
cor(data.frame(y=y, x1=x1, x2=x2))
summary(lm(y~x1+x2))
y
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
cor(data.frame(y=y, x1=x1, x2=x2))
summary(lm(y~x1+x2))
anova(lm(y~x1+x2))
summary(lm(y~x1))
anova(lm(y~x1))
summary(lm(y~x2))
anova(lm(y~x2))
x1=c(x1, 0.1)
x2=c(x2, 0.8)
y=c(y,6)
cor(x1,x2)
summary(y~x1+x2)
summary(lm(y~x1+x2))
anova(lm(y~x1+x2))
plot(y~x1+x2)
par(mfrow=c(2,2))
plot(lm(y~x1+x2))
summary(lm(y~x1))
anova(lm(y~x1))
par(mfrow=c(2,2))
plot(lm(y~x1))
summary(lm(y~x2))
anova(lm(y~x2))
par(mfrow=c(2,2))
plot(lm(y~x2))
